# Algoritmos

| Categoria                      | Nome do algoritmo          | Complexidade em lista de adjac√™ncias | Complexidade em matriz de adjac√™ncias |
| ------------------------------ | -------------------------- | ------------------------------------ | ------------------------------------- |
| Caminhamento                   | Depth First Search (DFS)   | $$O(V+E)$$                           | $$V^2$$                               |
| Caminhamento                   | Breadth First Search (BFS) | $$O(V+E)$$                           | $$V^2$$                               |
| Minimum Spanning Tree          | Kruskal                    | $$O(E \log E) + EV$$ ou s√≥ $$O(EV)$$ | $$O(E \log E) + EV$$                  |
| Minimum Spanning Tree          | Kruskal (Union Find)       | $$O(E \log E)$$                      | $$O(E \log E)$$                       |
| Minimum Spanning Tree          | Prim                       | $$O(V^2 + E)$$                       | $$O(V^2)$$                            |
| Minimum Spanning Tree          | Prim (Min-Heap)            | $$O(E \log V)$$                      |                                       |
| Minimum Spanning Tree          | Prim (Fib. Heap)           | $$O(E + V \log V)$$                  |                                       |
| Componentes Fortemente conexas | Kosaraju                   | $$O(V+E)$$                           | $$V^2$$                               |
| Fluxo                          | Ford-Fulkerson             | $$O(EF)$$                            |                                       |
| Fluxo                          | Edmonds-Karp               | $$O(VE^2)$$                          |                                       |
| Fluxo                          | Dinic                      | $$O(V^2 E)$$                         |                                       |
| Fluxo                          | Dinic Unitario             | $$O(\sqrt{V} E)$$                    |                                       |
| Shortest Path                  | Bellman-Ford               | $$O(VE)$$                            | $$O(V^2 E)$$                          |
| Shortest Path                  | Dijkstra                   | $$O(V^2)$$                           | $$O(V^2)$$                            |
| Shortest Path                  | Dijkstra (Min-Heap)        | $$O((V+E) \log V)$$                  |                                       |
| Shortest Path                  | Dijkstra (Fibonacci-Heap)  | $$O(E+V \log V)$$                    |                                       |

## Categoria de problemas

### Vizinhan√ßa

- **Descri√ß√£o:** encontrar todos os v√©rtices que s√£o vizinhos de um v√©rtice espec√≠fico.

- Matematicamente: $N(v) = \{u \in V(G) | uv \in E(G)\}$

- Lista de Adjac√™ncia: $O(1)$
  - Explica√ß√£o: basta acessar a lista de adjac√™ncias do v√©rtice.
  - Pseudoc√≥digo
    - **Entrada:** Grafo $G(V, E)$, v√©rtice $v \in V(G)$
    - **Sa√≠da:** $N(v)$
    - return $adj[v]$;

- Matriz de Adjac√™ncia: $O(V)$
  - Explica√ß√£o: √© necess√°rio percorrer a linha correspondente ao v√©rtice.
  - Pseudoc√≥digo
    - **Entrada:** Grafo $G(V, E)$, v√©rtice $v \in V(G)$
    - **Sa√≠da:** $N(v)$
    - *lst* $\leftarrow \emptyset$
    - **for** $u$ de $1$ at√© $n$ **do**
      - **if** $M[v][u]$ **then**
        - Inclui $u$ em *lst*;
      - **end**
    - **end**
    - return *lst*;

### Grau

- **Descri√ß√£o:** encontrar o n√∫mero de v√©rtices vizinhos de um v√©rtice espec√≠fico.

- **Matematicamente:** $d(v) = |N(v)|$

- **Lista de Adjac√™ncia:** $O(d(v))$
  - **Explica√ß√£o:** basta percorrer todos os elementos da lista de adjac√™ncias e contar quantos elementos existem.
  - **Pseudoc√≥digo**
    - **Entrada:** Grafo $G(V, E)$, v√©rtice $v \in V(G)$
    - **Sa√≠da:** $d(v)$
    - *lst* $\leftarrow adj[v].head;$
    - *deg* $\leftarrow 0;$
    - **while** *lst* $\neq \lambda$ **do**
      - *lst* $\leftarrow lst.next;$
      - *deg* $\leftarrow deg + 1;$
    - **end**
    - **return** *deg;*
  - [JV: Obs.: suponho que esse $\lambda$ seja um ponteiro nulo.]

- **Matriz de Adjac√™ncia:** $O(V)$
  - **Explica√ß√£o:** √© necess√°rio percorrer a linha correspondente ao v√©rtice e contar quantos elementos s√£o verdadeiros.
    - **Pseudoc√≥digo**
      - **Entrada:** Grafo $G(V, E)$, v√©rtice $v \in V(G)$
      - **Sa√≠da:** $d(v)$
      - *deg* $\leftarrow 0;$
      - **for** $u$ de $1$ at√© $n$ **do**
        - **if** $M[v][u]$ **then**
          - *deg* $\leftarrow deg + 1;$
        - **end**
      - **end**
      - **return** *deg;*

### Pertin√™ncia de Aresta

- **Descri√ß√£o:** verificar se uma aresta pertence ao grafo.

- **Matematicamente:** $uv \in E(G)$

- **Lista de Adjac√™ncia:** $O(d(u))$
  - **Explica√ß√£o:** basta percorrer a lista de adjac√™ncias do v√©rtice $u$ e verificar se $v$ est√° presente.
  - **Pseudoc√≥digo**
    - **Entrada:** Grafo $G(V, E)$, v√©rtices $u, v \in V(G)$
    - **Sa√≠da:** $uv \in E(G)?$ [JV: Booleano]
    - *lst* $\leftarrow adj[u].head;$
    - **while** *lst* $\neq \lambda$ **do**
      - **if** *lst* $= v$ **then**
        - **return** *true;*
      - **end**
      - *lst* $\leftarrow lst.next;$
    - **end**
    - **return** *false;*

- **Matriz de Adjac√™ncia:** $O(1)$
  - **Explica√ß√£o:** basta acessar a posi√ß√£o $M[u][v]$.
  - **Pseudoc√≥digo**
    - **Entrada:** Grafo $G(V, E)$, v√©rtices $u, v \in V(G)$
    - **Sa√≠da:** $uv \in E(G)$
    - **return** $M[u][v];$

<!-- ### Caminhamento -->

### Depth First Search (DFS)

- **Descri√ß√£o:** come√ßa em um v√©rtice inicial e segue para um dos seus vizinhos, continuando a explorar o pr√≥ximo v√©rtice n√£o visitado at√© que n√£o haja mais v√©rtices para visitar nesse caminho. Em seguida, ele retrocede e explora outros caminhos a partir dos v√©rtices anteriores.

- **Pseudoc√≥digo**
  - **Vari√°veis:**
    - $\pi[v]$: pai do v√©rtice $v$.
    - $i[v]$: tempo que encontramos o v√©rtice $v$. [JV: i de in√≠cio]
    - $f[v]$: tempo que terminamos de explorar o v√©rtice $v$. [JV: f de fim]
    - $cor$:
      - Branco: ainda n√£o visitei
      - Cinza: estou visitando
      - Preto: j√° terminei de visitar

  - **Algoritmo DFS base:** DFS(G, s):
    - **Entrada:** Grafo $G = (V, s)$, v√©rtice inicial $s$.
    - **para** $v$ at√© $V(G)$ **fa√ßa**
      - $cor[v] \leftarrow$ Branco;
      - $\pi[u] = \lambda;$ [JV: suponho que $\lambda$ seja um valor nulo]
    - **end**
    - $time \leftarrow 0;$
    - **para** $v \in V(G)$ **fa√ßa** [JV: ele for√ßa a DFS a percorrer todos os v√©rtices do grafo]
      - **se** $cor[v] = $ Branco **ent√£o**
        - $DFS-VISIT(G, u);$
      - **end**
    - **end**

  - **Algoritmo DFS-VISIT:** DFS-VISIT(G, u):
    - time $\leftarrow$ time + 1;
    - $cor[u] \leftarrow$ Cinza;
    - $i[u] \leftarrow$ time;
    - **para** $v \in N(u)$ **fa√ßa**
      - **se** $cor[v] = $ Branco **ent√£o**
        - $\pi[v] \leftarrow u;$
        - $DFS-VISIT(G, v);$
      - **end**
    - **end**
    - $time \leftarrow time + 1;$
    - $cor[u] \leftarrow$ Preto;
    - $f[u] \leftarrow$ time;

- Produtos da DFS:
  - **Grafo de predecessores:** $G_{\pi} = (V, E_{\pi})$ [JV: $E_{\pi}$ √© o conjunto de arestas que conectam os v√©rtices aos seus predecessores]
  - Os vetores de tempo de descoberta e de tempo em visita.

- **Complexidade:**
  - **Lista de Adjac√™ncia:** $\Theta(|V| + |E|)$
    - [JV: entendendo mais intuitivamente, podemos entender que percorreremos todos os v√©rtices, e para cada v√©rtice, percorreremos $d(v)$ arestas (Complexidade de se calcular $g(v)$ em uma lista de Adjac√™ncia). Ao final, teremos percorrido basicamente $2|E|$ arestas, visto que cada aresta est√° presente em duas listas de adjac√™ncias.]
  - **Matriz de Adjac√™ncia:** $\Theta(|V|^2)$
    - [JV: entendendo mais intuitivamente, numa matriz de adjac√™ncia, a complexidade para percorrermos os vizinhos de um v√©rtice √© $O(|V|)$; e como antes disso tamb√©m iremos executar o algoritmo para todos os v√©rtices, a complexidade total ser√° $O(|V|^2)$.]

- Classifica√ß√£o de arestas
  - **Arestas de √Årvore:** $vu \in G_{\pi}$
  - **Arestas de Retorno/Volta:** liga $u$ a um ancestral
  - **Arestas de Avan√ßo:** liga $u$ a um descendente
  - **Arestas de Cruzamento/Passagem:** as demais

```mermaid
graph LR
    %% Colora√ß√£o
    style A fill:#999, color:#000
    style B fill:#999, color:#000
    style C fill:#fff, color:#000

    %% V√©rtices
    A((A))
    B((B))
    C((C))

    %% Arestas
    A -->|√Årvore| B
    B -->|Retorno| A
    B -->|Avan√ßo| C
```

```mermaid
graph LR
    %% Colora√ß√£o
    style A fill:#000, color:#fff
    style B fill:#000, color:#fff
    style C fill:#999, color:#000

    %% V√©rtices
    A((A))
    B((B))
    C((C))

    %% Arestas
    A -->|√Årvore| B
    C -->|Passagem| B
```

### Ordena√ß√£o topol√≥gica

- **Descri√ß√£o:** dada um grafo ac√≠clico direcionado, a ordena√ß√£o topol√≥gica √© uma ordena√ß√£o linear dos v√©rtices tal que, para cada v√©rtice, todos os v√©rtices que ele aponta est√£o √† sua frente.

- **Algoritmo: Ordena√ß√£o_topol√≥gica(G)**
  - **Entrada:** Grafo $G(V, s)$
  - Use DFS para calcularos tempos de t√©rmino $f[v]$ para cada v√©rtice $v$.
  - **Ordena√ß√£o:** ordene os v√©rtices $V(G)$ em ordem decrescente de $f[v]$.

### Componentes Fortemente Conexas

<!-- #### Kosaraju -->

- **Descri√ß√£o:**
  - Dado um grafo direcionado $G = (V, A)$...
  - **Par de v√©rtices fortemente conexos:** s√£o v√©rtices $u$ e $v$ tais que √© poss√≠vel chegar de $u$ a $v$ e de $v$ a $u$.
  - **Grafo fortemente conexo:** √© um grafo em que todos os pares de v√©rtices s√£o fortemente conexos.
  - **Componente fortemente conexa:** √© um subgrafo fortemente conexo maximal.

```mermaid
graph LR
    A((A))
    B((B))
    C((C))
    D((D))

    A --> B --> C & D --> A
    B --> A
```

| conjunto       | Descri√ß√£o                    |
| -------------- | ---------------------------- |
| $[A, A]$       | Fortemente conexo (?)        |
| $[A, B]$       | Fortemente conexo            |
| $[A, B, C]$    | Grafo fortemente conexo      |
| $[A, B, C, D]$ | Componente fortemente conexa |

---

Definindo $G^T = (V, A^T)$, onde $A^T = \{(u, v) | (v, u) \in A\}$ (Conjunto de arcos de G com a dire√ß√£o trocada).

- **Algoritmo:**
  - **Entrada:** Grafos $G = (V, s)$
    - Use DFS para calcular os tempos de t√©rmino $f[v]$ para cada v√©rtice $v$.
    - Calcule $G^T$.
    - Use DFS em $G^T$ para visitar os v√©rtices em ordem decrescente de $f[v]$.

Com isso, teremos um vetor $\pi$ que indica os pais de cada v√©rtice. A quantidade de componentes fortemente conexas √© dada pela quantidade de v√©rtices que s√£o pais deles mesmos.

- **Complexidade:**
  - $O(DFS) + O(G^T) + O(Ordenar(f[v])) + O(DFS)$
  - $2* O(DFS) + O(G^T) + O(Ordenar(f[v]))$
  - $O(DFS) + O(G^T) + O(Ordenar(f[v]))$

### √Årvore Geradora M√≠nima (Minimum Spanning Tree - MST)

- $Peso(G) = \sum_{(u, v) \in E} w(u, v)$
  - Onde $w(u, v)$ √© o peso da aresta $(u, v)$.

#### Kruskal

- **Descri√ß√£o:** tenta sempre adicionar a aresta de menor peso, desde que n√£o crie um ciclo.

- **Algoritmo: Kruskal(G)**
  - **Entrada:** Grafo $G(V, E)$
  - **Sa√≠da:** √Årvore geradora m√≠nima $T$
  - $A \leftarrow \emptyset;$
  - $E \leftarrow ordena(E(G));$
  - **para todo** $uv \in E$ **fa√ßa**
    - **se** $uv + A$ n√£o forma ciclo **ent√£o**
      - $A \leftarrow A \cup \{uv\};$
    - **fim**
  - **fim**

- **Complexidade:**
  - $O(Ordena√ß√£o) + O(|E|)*(O(checa_ciclo) + O(insere_A))$
  - $O(Ordena√ß√£o) + O(|E|)*O(checa_ciclo) + O(|E|)*O(insere_A)$

Segundo o professor:

<!-- ### PEDIR AJUDA PRA ENTENDER ISSO -->

- Considerando que:
  - $X = O(checa_ciclo)$
  - $Y = O(insere_A)$
  - $m = |E|$

- **Lista de Adjac√™ncia:** $O(m \log n + mX + nY)$
- **Matriz de Adjac√™ncia:** $O(n^2 \log n + n^2X + nY)$

<!-- GPT: -->

Vamos analisar cuidadosamente o que pode estar faltando para voc√™ alcan√ßar a l√≥gica do professor em rela√ß√£o √† an√°lise da complexidade do algoritmo de Kruskal. Aqui est√£o os principais pontos a considerar e onde o entendimento pode ser refinado:

---

### 1. **Decomposi√ß√£o da Complexidade**

A complexidade total do Kruskal pode ser decomposta da seguinte forma:

$$O(Ordena√ß√£o) + O(E) \cdot (O(checa\_ciclo) + O(insere\_A))$$

Essa f√≥rmula √© v√°lida e faz sentido, mas o professor est√° especificando os tempos de execu√ß√£o de cada opera√ß√£o **dependendo da estrutura de dados utilizada** (lista de adjac√™ncia ou matriz de adjac√™ncia).

#### Passos detalhados

1. **Ordena√ß√£o das arestas**:
   - O n√∫mero de arestas √© \( m \) (ou \( |E| \)).
   - Ordenar as arestas pelo peso requer \( O(m \log m) \), mas \( m \leq n^2 \), ent√£o \( O(m \log m) = O(m \log n) \) usando \( \log m = \log n^2 = 2\log n \).

2. **La√ßo principal (itera√ß√µes sobre as arestas):**
   - A itera√ß√£o √© feita sobre \( m \) arestas.
   - Em cada itera√ß√£o, duas opera√ß√µes ocorrem:
     - **Checar se adicionar uma aresta forma um ciclo** (\( O(checa\_ciclo) \)).
     - **Inserir a aresta na solu√ß√£o \( A \)** (\( O(insere\_A) \)).

---

### 2. **Checar Ciclos (\( O(checa\_ciclo) \))**

O professor usa o contexto do **Union-Find** para determinar a presen√ßa de ciclos. No Union-Find, o custo de checar ciclos depende de:
- **Busca (find):** Para verificar se os dois v√©rtices de uma aresta pertencem ao mesmo conjunto.
- Esse custo √© **quase constante** devido √† compress√£o de caminhos e √† uni√£o por tamanho/classifica√ß√£o:
  \[
  O(checa\_ciclo) = O(\alpha(n)), \text{ onde } \alpha(n) \text{ √© a inversa da fun√ß√£o de Ackermann}.
  \]
- Como \( \alpha(n) \) √© extremamente pequeno, podemos aproxim√°-lo como **constante** para prop√≥sitos pr√°ticos.

#### O que isso significa?
- O termo \( O(E) \cdot O(checa\_ciclo) \) reduz para algo **pr√≥ximo de \( O(E) \)**, mas ele ainda o deixa como \( mX \) para indicar depend√™ncia do custo espec√≠fico do ciclo.

---

### 3. **Inserir na Solu√ß√£o (\( O(insere\_A) \))**

A opera√ß√£o de inserir uma aresta na solu√ß√£o \( A \) pode ter custos diferentes dependendo da estrutura de dados usada para armazenar \( A \):
- Se for uma **lista de adjac√™ncia**, a inser√ß√£o √© \( O(1) \) para cada aresta. Logo, \( O(insere\_A) = O(1) \), e o custo total ser√° \( O(m) \).
- Se for uma **matriz de adjac√™ncia**, a atualiza√ß√£o pode exigir opera√ß√µes mais complexas, como atualizar \( O(n) \) posi√ß√µes (uma linha ou coluna inteira), dependendo da implementa√ß√£o.

O professor provavelmente est√° generalizando isso como \( nY \), pois \( Y \) pode variar dependendo da estrutura.

---

### 4. **Como a Estrutura de Dados Afeta o C√°lculo?**

Agora vamos interpretar as f√≥rmulas que o professor apresenta para as duas representa√ß√µes:

#### **Lista de Adjac√™ncia:**

\[
O(m \log n + mX + nY)
\]
- \( m \log n \): Ordena√ß√£o das \( m \) arestas.
- \( mX \): Custo de checar ciclos para \( m \) arestas.
- \( nY \): Inser√ß√£o das arestas na solu√ß√£o (aqui \( Y \) √© provavelmente \( O(1) \), ent√£o o custo total √© dominado por outros termos).

#### **Matriz de Adjac√™ncia:**
\[
O(n^2 \log n + n^2X + nY)
\]
- \( n^2 \log n \): Ordena√ß√£o, porque no caso pior \( m = O(n^2) \).
- \( n^2X \): Checar ciclos para \( n^2 \) arestas.
- \( nY \): Inser√ß√£o das arestas na matriz (provavelmente maior que para lista).

---

### 5. **O que pode estar faltando?**

Talvez o que precise de mais aten√ß√£o para "fechar" o racioc√≠nio seja entender as seguintes quest√µes:

1. **Impacto da estrutura de dados no custo das opera√ß√µes**:
   - Como a lista de adjac√™ncia e a matriz de adjac√™ncia afetam \( O(checa\_ciclo) \) e \( O(insere\_A) \).
   - Na lista de adjac√™ncia, o custo de inserir arestas ou verificar ciclos √© mais eficiente do que na matriz.
2. **Generaliza√ß√£o de \( X \) e \( Y \)**:
   - \( X \) pode ser tratado como \( O(\alpha(n)) \) devido ao Union-Find.
   - \( Y \) √© normalmente \( O(1) \), mas pode variar se a matriz exigir atualiza√ß√µes mais pesadas.

3. **Separa√ß√£o de termos \( m \) e \( n^2 \)**:
   - Na matriz, \( m \) pode crescer at√© \( n^2 \), ent√£o o custo \( O(n^2 \log n) \) passa a dominar.

Se precisar de mais detalhamento em algum ponto espec√≠fico, posso te ajudar a refor√ßar! üòä

#### Prim

### Breadth First Search (BFS)

### Fluxo

#### Ford-Fulkerson

start with 0 flow
while there exists an augmenting path: // iterative algorithm
‚Äâ‚Äâfind an augmenting path (for now, 'any' graph traversal will do)
‚Äâ‚Äâcompute bottleneck capacity
‚Äâ‚Äâincrease flow on the path by the bottleneck capacity

#### Edmonds-Karp

Implementation: We first ignore capacity of the edges first (assume all edges in the residual graph have weight 1), and we run O(E) BFS to find the shortest (in terms of # of edges used) augmenting path. Everything else is the same as the basic Ford-Fulkerson Method outlined earlier.

It can be proven that Edmonds-Karp will use at most O(VE) iterations thus it runs in at most in O(VE * E) = O(VE^2) time.

#### Dinic

Dinic's algorithm also uses similar strategy of finding shortest augmenting paths first.

But Dinic's algorithm runs in a faster time of O(V^2 √ó E) due to the more efficient usage of BFS shortest path information.

This slide will be expanded.

### Shortest Path

#### Bellman-Ford

#### Dijkstra
