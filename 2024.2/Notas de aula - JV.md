# PAA 2024 - Marcio Costa Santos <marciocs@dc.ufmg.br> & Olga NIkolaevna Goussevskaia

livro do cormen e do udi manber

## M√≥dulo 1

### Aula 1 - 25/09/24

Primeira aula √© pra explicar

- An√°lise e complexidade de algoritmos
- Algoritmos em grafos
- Projetos e Design de algoritmos
- Complexidade de problemas

Os dois primeiros √© dele. As duas √∫ltimas √© com a Olga.

Cada um dos m√≥dulos vale 25 pontos. O primeiro m√≥dulo s√≥ tem uma prova que vale 25. Nos outros ter√° uma prova mais uma atividade, podendo ser uma lista de exerc√≠cios ou um trabalho pr√°tico. Cada professor define de que forma distribui os 25 pontos entre prova e atividade.

Muito provavelmente pra ele ser√° 20 pra prova e 5 pra atividade/lista. Geralmente a Olga faz (20 e 5), e (15, 10)

V√¢o postar o cronograma de PAA no Moodle.

Algumas das avalia√ß√µes s√£o tamb√©m pelo Moodle

Ele geralmente usa slide e tamb√©m escreve muito no quadro. Algumas coisas como algoritmos e desenhos ele prefere ter no slide.

Os slides e as listas ele disponibiliza no Moodle, mesmo que ele n√£o fa√ßa atividades s√≥ de exerc√≠cios. Ele deve fazer duas aulas assim antes de cada uma das provas dos dois primeiros m√≥dulos. Por√©m, tente resolver todos os exerc√≠cios antes das aulas.

Pretende enviar todo o material do m√≥dulo de uma s√≥ vez.

Ele segue um livro e a Olga, outro. Ele gosta de seguir o Cormen "de cabo a rabo".

Ele vai indicar quais cap√≠tulos correspondem √†quela aula.

#### Sobre o m√≥dulo

Ela n√£o √© uma disciplina pra aprender a programar, nem √© das mais f√°ceis de teoria.

Exemplo que ele vai fingir que vc sabe: PA, PG, Logaritmo; Estruturas de controle em programa√ß√£o; Estruturas de dados b√°sicos (filha, pilha, √°rvore, heap);

A disciplina √© de An√°lise e constru√ß√£o de algoritmo. A ideia do curso √© que voc√™ j√° entende o b√°sico e vai partir da√≠.

O professor Coutinho criou um material de estudo que √© o "Pr√©-PAA" que tem uma sequ√™ncia de materiais para estudo pr√≥prio. Se n√£o souber de algo, pare e tente resolver.

√â importante seguir o cronograma e fazer isso o mais r√°pido poss√≠vel.

As aulas v√£o come√ßar b√°sicas e elas tendem a se tornar muito complicadas.

Exemplo: Aula 1 √© o b√°sico de grafos. Aula 2 j√° seria a nota√ß√£o matem√°tica para o que foi dado na aula 1.

Ele deve usar bastante transforma√ß√£o de logaritmo.

- Que linguagem de programa√ß√£o √© usada em PAA?
  - Nenhuma. O que ele vai mostrar s√£o algoritmos. Exemplos: `while (deu certo)`, `Inclui V NaLista L`, `R = folhas da √°rvore T`.
  - No in√≠cio ele deve tentar esquematizar mais a parte da estrutura dos dados, mas com o tempo vai se tornar cada vez mais abstrato.

Especialmente em algoritmos recursivos ele vai explicar como funciona o algoritmo, principalmente na ideia, mas faltando alguns passos. Exemplo: ignorando todos os corner-cases e focar na ideia que √© principal.

Python, Java, C ou C++. Ele √© muito fanboy de Julia kkkkkkkkkkkkkkkkkkkkkk

O trabalho provavelmente vai envolver alguma coisa de programa√ß√£o.

- Ele mais se importa que a ideia do algoritmo esteja certa. N√£o importa muito os pormenores. Aqueles √∫ltimos detalhes:
  - Divide em grupos de cinco
    - E se n√£o for m√∫ltiplo?
  - Divide na metade
    - E se for √≠mpar?

A primeira parte da disciplina n√£o √© criar algoritmos, apenas analis√°-los.

Nessa primeira parte √© aquela an√°lise de Big-O para definir a quantidade de passos que o algoritmo executa.

Geralmente vai se utilizar formas matem√°ticas de se utilizar Somat√≥rios, que geralmente s√£o PAs ou PGs.

- √Äs vezes o somat√≥rio √© muito esquisito, que resulta em algo ainda mais esquisito. Exemplo:
  - $\sum^{n}_{i = 0} = \frac{1}{i} = \log n$
  - Mas nesses casos ele geralmente informa que esses "passes de m√°gica" ser√£o utilizados nas provas.

Recurs√£o ser√° muito utilizado. Mesmo algoritmos n√£o recursivos ele tende a escrev√™-las de forma recursiva. Ent√£o, teremos que analisar bem cuidadosamente a complexidade de algoritmos recursivos.

Veremos dois m√©todos para a an√°lise de algoritmos recursivos.

Muitos dos resultados ele tentar√° provar matematicamente que algumas an√°lises est√£o corretas.

Faremos algumas demonstra√ß√µes na disciplina, especialmente nos m√≥dulos 3 e 4, principalmente no 4.

Um dos principais √© prova por indu√ß√£o. Ele tentar√° j√° utilizar algumas provas por indu√ß√£o, especialmente as provas que envolvem grafos na segunda parte da disciplina.

Na primeira parte √© mais conta.

Ele vai tentar n√£o exigir provas matem√°ticas nas avalia√ß√µes, mas geralmente tem na lista. Mesmo que n√£o precise demonstrar, geralmente √© √∫til que se saiba como que acontece para ficar mais f√°cil de entender certas coisas.

Geralmente os alunos tem mais dificuldade nos algoritmos recursivos.

A disciplina ser√° muito corrida √© o tempo da disciplina √© muito curto. Raramente ele demorar√° mais do que uma aula por assunto.

Raramente eles voltar√£o nos assuntos das aulas anteriores.

Se perder alguma aula (Por favor n√£o fa√ßa isso). √â a pior coisa que voc√™ pode fazer nessa disciplina.

Os m√≥dulos geralmente s√£o bem desconexos. Mas num mesmo m√≥dulo, se perder algo, dificilmente vai conseguir recuperar essa informa√ß√£o. Se perdeu algo, vai ter que refazer todas as aulas perdidas antes das aulas posteriores. No primeiro m√≥dulo ainda d√° pra faltar algumas, mas nas pr√≥ximas... üíÄ

Quem n√£o tem acesso ao Moodle, tenha o mais r√°pido poss√≠vel.

Sexta j√° deve ter todos os materiais l√°.

Existe a chance de que mude de sala. E se avisar, ser√° pelo moodle.

#### D√∫vidas dos alunos

- Tem monitor?
  - Ele tem X% de certeza que sim, mas ainda n√£o temos. At√© o come√ßo de outubro devem saber quem √©. Geralmente ser√£o na sexta que √© quando estamos mais livres. Ele recomenda fortemente que venhamos nas monitorias. Ele gosta de postar uma lista por semana. √Ä priori elas n√£o valem ponto.
- Enquanto n√£o tem monitor, com quem tirar d√∫vidas?
  - O M√°rcio geralmente tem as manh√£s livres, depois das 8h30 na sala dele 4322 no anexo do DCC. Normalmente √© s√≥ chegar l√° e bater. Sexta ele geralmente t√° aqui o dia todo. Se for algo que demore mais, pode mandar e-mail.
- Vai ter alguma aula de pr√©-PAA ou vai direto na mat√©ria?
  - Ele vai direto na mat√©ria, at√© porque a mat√©ria √© muito extensa.
- Tem v√≠deo-aulas gravadas?
  - Acha que sim, provavelmente o Gabriel Coutinho tem.
- As datas das provas do cronograma est√£o definidas a serem nesses dias mesmo?
  - Sim. Dificilmente mudar√£o a n√£o ser que aconte√ßa algo mt at√≠pico (pandemia/greve). Se acontecer algo com as aulas, ainda d√° pra repor em outros hor√°rios, mas prova √© pouco prov√°vel que se altere.
- A substitutiva [funciona como]?
  - Se voc√™ perder uma avalia√ß√£o, ele sempre abre uma substitutiva para cobrir uma que vc perdeu. Ele costuma deixar que vc fa√ßa a substitutiva mesmo que tenha feito as provas. E a nota da substitutiva sobrescreve a que voc√™ j√° fez, independente de ser maior ou menor.
  - A substitutiva ocorre no s√°bado. Sempre s√°bado de manh√£, normalmente √†s 9h. As provas geralmente s√£o nos hor√°rios de aula. A Olga geralmente prefere fazer as provas no s√°bado tamb√©m.
- O conte√∫do da substitutiva √© qual?
  - O conte√∫do vai ser de acordo com a prova que voc√™ quer substituir.
- Alguma pergunta que n√£o vi
  - Ele vai dizer quais v√£o ser os cap√≠tulos que ele dar√° em cada aula. O livro √© da terceira vers√£o.
- Qual a diferen√ßa entre "PG2" e "PG1_PG2 - Metaturma"?
  - Nenhuma. S√£o duas turmas diferente acontecendo no mesmo hor√°rio, com o mesmo professor, na mesma sala. A Meta-turma √© para reencaminhar os conte√∫dos para as turmas menores.
- E onde enviaremos as atividades?
  - Na Metaturma.
- Qual o cap√≠tulo da aula de segunda?
  - Se n√£o se engana, os cap√≠tulos 2 e 3. E veja tamb√©m os conte√∫dos de pr√©-PAA.

### Aula 2 - 30/09/2024 - [13h08, 14h40]

#### Slide: Complexidade de Algoritmos

- Algoritmos no geral ser√£o considerados como fun√ß√µes $f(n)$ que transformam conjunto de entrada em conjunto de sa√≠da.
- Para descrever os algoritmos, ser√£o utilizados pseudoc√≥digos de forma imperativa com estruturas usuais de controle de fluxo.
- atribui√ß√£o como setas e iguais como compara√ß√£o
- Estruturas de dados simples.
- Considera-se mem√≥ria infinita sem se preocupar com a atribui√ß√£o.

D√∫vida 1: Exemplo algoritmo 1: o i=1 seria uma compara√ß√£o ou atribui√ß√£o?

- D√∫vida 2: Exemplo algoritmo 2: Por que a sa√≠da t√° como "?"?
  - Resposta: Para pensarmos sobre o que o algoritmo t√° fazendo, sem dar a resposta de cara.

- Todos os algoritmos ignoram completamente quaisquer dos cornercases que poderiam dar errado: overflow, aloca√ß√£o de mem√≥ria, typechecking, etc.
- O problema do m√≥dulo 1 √© analisar qu√£o bons ou ruins s√£o os algoritmos. Suponho eu que seja a nota√ß√£o de Big-O.
- A complexidade de um algoritmo √© uma fun√ß√£o que descreve o n√∫mero de opera√ß√µes elementares que o algoritmo executa em fun√ß√£o do tamanho da entrada.
- "Custos"
  - Se = Escolha, subconjunto. [Considero que seja algo do tipo um "ou", ou uma multiplica√ß√£o entre as possibilidades.]
  - Para e enquanto: Somat√≥rio
  - Atribui√ß√£o: tempo unit√°rio
  - Matem√°tica e regras: tempo unit√°rio  (a depender da complexidade das regras)
  - Estruturas de dados: Tempo de cada opera√ß√£o.
- O la√ßo tem custo de "2" porque incrementa e compara se chegou na condi√ß√£o.
- No caso do Algoritmo 1, a Fun√ß√£o seria: $F_1(x, n) = 1 + \sum_{i=1}^{n} (2 + 3) = 1 + \sum_{i=1}^{n} (5) = 1 + 5n$.
- No caso do Algoritmo 2, a Fun√ß√£o seria: $F_2(x, n) = 2 + \sum_{i=2}^{n} (1 [checagem do loop] +2 [compara√ß√£o condicional + indexa√ß√£o] +1 [somat√≥rio do iterador]) + \sum_{i=2; se x[i] for par}^{n} (1) [CASOS EM QUE OCORRE A OPERA√á√ÉO *SE*] = 2 + (n-1)*4 [N-1 porque come√ßou i pelo valor 2] + \sum_{i=2; se x[i] for par}^{n} (1)$.
  - Poderia tamb√©m utilizar algo como $\sum_{i=2; se x[i] for par}^{n} (1) = (x[i]\%2)*1$
- Exemplo Algoritmo 3:
  - 1: n*m: preencher a matriz Z NxM
  - 2: compara√ß√£o e incremento do loop i
  - 2: compara√ß√£o e incremento do loop j
  - 8: 2: indexa√ß√£o $z[i][j]$, 2: indexa√ß√£o $x[i][j]$, 2: indexa√ß√£o $y[i][j]$, 1: soma x e y, 1: atribui√ß√£o em $z[i][j]$
  - $F_3(x, y, n, m) = n*m + \sum_{i=1}^{n} (2 + \sum_{j=1}^{m} (2+8)) = n*m + \sum_{i=1}^{n} (2 + 10m) = n*m + 2n + 10nm = 11nm + 2n$

- Inst√¢ncia: conjunto de dados de entrada de um algoritmo: $I$
- Tamanho de uma inst√¢ncia: tamanho em bits da entrada: $I_n$
- Complexidade de um algoritmo: √© a fun√ß√£o que leva o tamanho da inst√¢ncia em...

- Complexidade de pior caso: o maior n√∫mero de passos para uma inst√¢ncia de tamanho $n$.
  - $T(n) = \max_{x \in I_n} F(n, x)$
- Complexidade de melhor caso: o menor n√∫mero de passos para uma inst√¢ncia de tamanho $n$.
  - $T(n) = \min_{x \in I_n} F(n, x)$

D√∫vida: Existe um c√°lculo estat√≠stico de qu√£o prov√°veis de ocorrer s√£o os melhores e maiores casos?
Resposta: Pelo que eu entendi, at√© d√°, s√≥ que √© bem dif√≠cil calcular

- Complexidade de m√©dio caso: o n√∫mero esperado de passos para uma inst√¢ncia de tamanho $n$.
  - $T(n) = \sum_{x \in I_n} P(x)F(n, x)$
  - $P(x)$ √© a probabilidade de ocorrer a inst√¢ncia $x$.
    - "Mas como calcular a probabilidade de uma inst√¢ncia?" "N√£o √© t√£o f√°cil assim"
- Ele sempre considerar√° "complexidade" como sendo "complexidade de pior caso".

- Algoritmo 1:
  - Melhor: [...] 1+5n
  - Pior:  [...] 1+5n
  - M√©dio:  [...] 1+5n
    - Entendi +- como que o F(x, n) foi pra fora do somat√≥rio.
- Algoritmo 2:
  - Melhor: todos elementos s√£o √≠mpares [...] 5n - 4
  - Pior: todos elementos s√£o pares [...] 7n-6
  - M√©dio:  [...] M√≥ trampo. Favor ignorar üòÑüëç

- An√°lise Assint√≥tica
  - O objetivo √© analisar o comportamento de uma fun√ß√£o quando $n$ tende ao infinito.

- D√∫vida: Por que eu compararia n=infinito do pior com o n=infinito do melhor?
- Resposta: Porque no caso, o que a gente t√° comparando √© a melhor e pior distribui√ß√£o dos valores para uma mesma quantidade de elementos. Ent√£o, a gente t√° comparando o melhor caso de uma quantidade de elementos com o pior caso de uma quantidade de elementos.

- Simbolos:
  - $O$
  - $o$
  - $\Theta$
  - $\Omega$

- f = G(g). Essa parte ficou Muito confusa.
- f = O(g) Existem $n_0$ e $c$ tal que: $f(n) \leq c*g(n)$ para todo $n \geq n_0$
  - Entende-se o $c$ como sendo uma forma de chutar o valor de $g$ para cima. E o $n_0$ indica o momento em que $f$ come√ßa a ser menor que $g$.

- Geralmente e procura o menor limite superior assint√≥tico, mas usar outros maiores tamb√©m √© v√°lido. (Menos na prova)
  - $N^k + N^{k-1} \dots + N + 1 = O(N^k)$

Alguns exerc√≠cios ser√£o mostrar valores $C$ e $N_o$ que satisfa√ßam a equa√ß√£o e provem o limite superior.

Geralmente o que ele vai pedir √© encontrar o O() de uma fun√ß√£o.

### Aula 3 - 02/10/2024 - [13h06, 14h40]

- Aulas extras
  - 13h √†s 15h
  - N√£o tem presen√ßa
  - N√£o precisa dos conte√∫dos, mas os conte√∫dos podem ser √∫teis.
  - Ir√£o confirmar quais ser√£o os t√≥picos de cada aula
  - Pontua√ß√£o extra por presen√ßa nas aulas. Muito provavelmente 0,5 ponto por aula.
  - Quase como se fosse aula de pr√©-PAA

#### Aula 3: Slide - Aula 2 - Complexidade e Nota√ß√£o Assint√≥tica

- Limite superior
  - $f() = O(g())$ pode ser r√∫sticamente definido como $f() \leq g()$
- Limite superior estrito
  - $f() = o(g())$ pode ser r√∫sticamente definido como $f() < g()$
  - $f = o(g)$ Para todo $c > 0$ existe $n_0$ tal que: $f(n) < c*g(n)$ para todo $n > n_0$
  - √â importante analisar matematicamente de que forma que o descubramos quais os poss√≠veis valores de $C$ e seu respectivo $n_0$.
  - Podemos entender que $n \neq o(n)$ e que $n = O(n)$
- Limite Inferior
  - $f() = \Omega(g())$ pode ser r√∫sticamente definido como $f() \geq g()$
  - Existem $n_0$ e $c$ tal que: $f(n) \geq c*g(n)$ para todo $n \geq n_0$
- Limite Inferior Estrito
  - $f() = \omega(g())$ pode ser r√∫sticamente definido como $f() > g()$
  - Para todo $c > 0$ existe $n_0$ tal que: $f(n) > c*g(n)$ para todo $n > n_0$
- Equival√™ncia
  - $f() = \Theta(g())$ pode ser r√∫sticamente definido como $f() = g()$
  - Existem $n_0$, $c_1$ e $c_2$ tal que: $c_1*g(n) \leq f(n) \leq c_2*g(n)$ para todo $n \geq n_0$
  - Que equivale a dizer que $f() = \Omega(g())$ e $f() = O(g())$
  - Exemplos: $2n^2 + n = \Theta(n^2)$
  - Geralmente busca-se o $\Theta$ de uma fun√ß√£o, mas foca-se mais no $O$.

- Propriedades
  - $k*f(n) = O(f(n))$ para todo $k \in \R$
  - $f(n) * g(n) = O(f(n) * g(n))$
  - $O(f(n)) + O(g(n)) = O(f(n) + g(n))$
  - $O(f(n)) * O(g(n)) = O(f(n) * g(n))$
  - Extra: $O(n)*n = O(n*n) = O(n^2)$

- Entende-se que $O(f(n)) = [conjunto] {g \in F | g = O(f(n))}$
  - Sendo $F$ todas as fun√ß√µes poss√≠veis

- √â importante considerar que os somat√≥rios entre fun√ß√µes √© o somat√≥rio normal. E o somat√≥rio entre $O()$'s √© um tipo de agrupamento entre conjuntos de fun√ß√µes.
- Na prova sempre considerar que estamos buscando o limite mais estrito poss√≠vel.
- Quando n√£o se tem condicional, podemos considerar que o $O()$ √© o mesmo que o $\Omega()$, que s√£o iguais ao $\Theta()$.

#### Aula 3: Slide - Aula 3 - Algoritmos Recursivos e Rela√ß√µes de Recorr√™ncia

[JV: O que √© fun√ß√£o de recorr√™ncia?
R Copilot: √â uma fun√ß√£o que √© definida em termos de si mesma. Exemplo: $f(n) = f(n-1) + 1$]

- Sempre consideraremos que $F(1)$ e $F(0)$ s√£o constantes e iguais a $O(1)$, sendo $F$ uma fun√ß√£o recursiva.

- Algoritmo Recursivo 1

```pseudocode
Algoritmo 1: REC
Entrada: inteiro x.
Sa√≠da: ?
se x <= E REC(x-1) >= 1 ent√£o
  RETORNA REC(x-1) + 1
RETORNA 1;  
```

"A parte de encontrar a fun√ß√£o de recorr√™ncia para o algoritmo √© tranquila. O problema √© a an√°lise da parte recursiva"

- Resolu√ß√£o de Recorr√™ncias
  - Podemos resolver equa√ß√µes na forma:
    1. $T(n) = aT(n/b) + f(n)$
        - Num caso de divis√£o e conquista, a parte da divis√£o seria o $b$, e a agrega√ß√£o das respostas seria o $a$.
    2. $T(n) = aT(n-b) + f(n)$
        - Esse geralmente se refere a casos em que v√° removendo alguns itens de uma estrutura de dados a cada passo.
  - N√£o podemos resolver equa√ß√µes na forma:
    - 3. $T(n) = T(n-a) + T(n-b)$
      - [Tipo Fibonacci]
      - Tendem  a ser ineficientes

$F(n) = F(n-1) + 1$; Condi√ß√£o de parada: $F(1) = 1; F(0) = 1$

$$
F(n) =\\
F(n-1) + 1 =\\
(F(n-2) + 1) + 1 = F(n-2) + 2 =\\
(F(n-3) + 1) + 1 + 1 = F(n-3) + 3 =\\
\vdots\\
F(n) = F(n-k) + k\\
\text{Para n = k}\\
F(k) = F(0) + n = O(1) + n = O(n)
$$

---

$F(n) = F(n-1) + n$

$$
F(n) = F(n-1) + n =\\
(F(n-2) + n-1) + n = F(n-2) + 2n - 1 =\\
(F(n-3) + n-2) + n-1 = F(n-3) + 3n - 2 - 1 =\\
\vdots
F(n-k) + kn - \sum_{i=1}^{k} i=\\
F(0) + n^2 - \sum_{i=1}^{n-1} i \implies \frac{(n-1)*n}{2} \implies O(n^2)
F(0) + n^2 - O(n^2) = O(n^2)
$$

"Teorema mestre"

Revisar PA e PG.

---

D√∫vida: Quando na soma assint√≥tica h√° uma subtra√ß√£o, a gente ignora ela? Sim. Considera-se como se fosse soma normal. Mas a mesma ignor√¢ncia n√£o ocorre na divis√£o e multiplica√ß√£o

#### Aula 4: Slide - Aula 3 - Algoritmos Recursivos e Rela√ß√µes de Recorr√™ncia

- $T(n) = T(n - b) + f(n)$
- $T(n) = a * T(\frac{n}{b}) + f(n)$

- Exemplo:
  - $T(n) = 2T(n-4)+5$
    - $T(0) = O(1)$
    - $T(1) = O(1)$
    - $T(2) = O(1)$
    - $T(3) = O(1)$
  - Ele n√£o precisa especificar, nem pretende, mas a ideia √© que todas tenham esses crit√©rios de parada.
  - Resolvendo pelo m√©todo da substitui√ß√£o:

$$
T(n)   = 2( T(n-4)    ) + 5 \\
T(n-4) = 2(2T(n-8) + 5) + 5 = 2^2 T(n-8) + 5*2 + 5 \\
T(n-8) = 2^2 (2T(n-12) + 5) + 5*2 + 4 =  \\
\vdots \\
\text{PASSO K:} 2^k T(n- k*4) + \sum_{i=0}^{k - 1} 2^i*5
$$

- Caso de t√©rmino: n - k*4 = 0; 4k = n; k = n/4
- Complexidade: $2^(n/4) * \Theta(1) + 5 * \sum_{i=0}^{n/4 - 1} 2^i = \Theta(2^{n/4}) + 2^{n/4} - 1 = \Theta(2^{n/4})$

N√£o mexer com constantes em expoentes

- Teorema Mestre
  - Sejam $a \geq 1$ e $b > 1$ constantes, $f(n)$ uma fun√ß√£o, e $T(n) = aT(\frac{n}{b}) + f(n)$, ent√£o, para algum $\epsilon > 0$:
    - Se $f(n) = O(n^{\log_{b}(a) - \epsilon}) \implies T(n) = \Theta(n^{\log_{b}(a)})$ leq
    - Se $f(n) = \Theta(n^{\log_{b}(a)}) \implies T(n) = \Theta(n^{\log_{b}(a)} * log(n))$ =
    - Se $f(n) = \Omega(n^{\log_{b}(a) + \epsilon})$ e $a f (\frac{n}{b}) \leq cf(n)$ ent√£o $\implies T(n) = \Theta(f(n))$ geq

Esse $\epsilon$ quando usa a propriedade de pot√™ncia, torna o caso do Big-O numa divis√£o e o caso do Big-Omega numa multiplica√ß√£o.

- Revisar propriedade de logaritmo

Ele fez dois exemplos no quado que eu n anotei.

- $T(n) = 2T(\frac{n}{2}) + 1$
  - $a = 2; b = 2; f(n) = 1$
  - $\log_{b}(a) = \log_{2}(2) = 1$
  - $n^{\log_{b}a = n^1 = n}$
  - $f(n) = O(n^{\log_{b}(a)})$
  - $1 = O(n^{1-\epsilon}) = \Theta(n)$
- $T(n) = 4T(\frac{n}{4}) + n + 1$
  - $a = 4; b = 4; f(n) = n + 1$
  - $\log_{b}(a) = \log_{4}(4) = 1$
  - $n^{\log_{b}a} = n^1 = n$
  - $f(n) = \Theta(n^{\log_{b} a})$
  - $n+1 = \Theta(n)$
  - $\Theta (n * log n)$

Exerc√≠cio: Descobrir pq que $\log_{b}(n) = O(\log_{2}(n))$ para $b > 1$

- $T(n) = 4T (\frac{n}{2}) + n^3 + n - 5$
  - $a = 4; b = 2; f(n) = n^3 + n - 5$
  - $\log_{b}(a) = \log_{2}(4) = 2$
  - $n^3 + n - 5 = \Omega(n^{2+\epsilon})$
  - $\Theta(f(n)) = \Theta(n^3 + n - 5) = \Theta(n^3)$
- $T(n) = 4T (\frac{n}{2}) + n^2\log_{{2}}(n)$: N√£o d√° pra usar o teorema mestre nessa fun√ß√£o.
  - Tentando usar o teorema mestre
  - $a = 4; b = 2; f(n) = n^2\log_{2}(n)$
  - $\log_{b}(a) = \log_{2}(4) = 2$
  - $n^{\log_{b}(a)} = n^2$
  - $n^2$ e $n^2 * log(n)$ s√£o equivalentes?
  - ... [Tentativa de resolu√ß√£o pelo Teorema Mestre]

Na prova ele espera que se diga "n√£o d√° para usar Teorema Mestre". Ele n√£o espere que se resolva a equa√ß√£o. At√© pode sair abrindo se preferir.

O problema surge quando comparamos o $f(n)$ com a equa√ß√£o com $a$ e $b$ e que a √∫nica diferen√ßa seja o log.

"Tem que ser um polin√¥nimo menor ou maior para ter diferen√ßa entre elas."

Algumas quest√µes n√£o v√£o dar pra fazer. √Äs vezes tem como, mas √© meio tortuoso.

- Exemplo: $T(n) = T(\frac{n}{3}) + T(\frac{n}{4}) + 1$
  - Divide em duas equa√ß√µes falsas do tipo:
    - $F(n) = F(\frac{n}{3}) + F(\frac{n}{3}) + 1 = 2F(\frac{n}{3}) + 1$
    - $G(n) = G(\frac{n}{4}) + G(\frac{n}{4}) + 1 = 2G(\frac{n}{4}) + 1$

- Vai ter gabarito das listas?
  - +-, ele vai tirar d√∫vidas no final. Teremos monitoria e a monitora deve responder boa parte das respostas da lista.

"Essa √© a parte mais f√°cil da mat√©ria"

"Estude isso *freneticamente*"

### Aula 4 - 09/10/2024 - [13h08, 14h47]

#### Pr√©-aula 4

- Monitoria possivelmente remota e/ou presencial
- BeeCrowd para trabalhos
- Ele adiantar√° a aula de exerc√≠cios e provavelmente ser√£o 3 horas no s√°bado.
- Alguns t√≥picos extras nas aulas de sexta

#### Slide: An√°lise Amortizada

##### Contador Bin√°rio

- Assume que temos um vetor de $n$ bits.
- Este vetor representa um n√∫mero.
- Vamos criar uma fun√ß√£o para incrementar uma unidade no n√∫mero representado.

$$
{1, 3, 4} =
\begin{bmatrix}
  5 && 4 && 3 && 2 && 1 && 0\\
  0 && 1 && 1 && 0 && 1 && 0\\
\end{bmatrix} =
26
$$

$$
{2} =
\begin{bmatrix}
  4 && 3 && 2 && 1 && 0\\
  1 && 1 && 0 && 1 && 0\\
\end{bmatrix} =
4
$$

---

- 0: 0000
- 1: 0001
- 2: 0010
- 3: 0011
- 4: 0100
- 5: 0101
- 6: 0110
- 7: 0111
- 8: 1000

O que o algoritmo faz?

- Enquanto tem um, troca pra zero, quando achar o primeiro zero troca pra um, o resto n√£o faz nada.

##### Algoritmo

...

##### Contador Bin√°rio - Complexidade

- Complexidade de Pior caso: O(n).
- Mas esse pior caso acontece muito raramente...
- As opera√ß√µes t√™m uma rela√ß√£o clara entre elas.
- Seria interessante ter uma liga√ß√£o entre a complexidade e as opera√ß√µes

##### Contador Bin√°rio - An√°lise Amortizada

- Considere o n√∫mero de opera√ß√µes para se realizar uma sequ√™ncia de n opera√ß√µes: $T(n)$.
- Desejamos calcular
  - $\frac{T(n)}{n}$
- Complexidade Amortizada.

D√∫vida: qu√£o distante a complexidade amortizada est√° do caso m√©dio?
Resposta: Razoavelmente distante. Porque nesse caso o output de um √© o input do outro. J√° no caso m√©dio, sorteia-se n√∫meros aleat√≥rios como input.

- Tr√™s m√©todos:
  - (esqueci)
  - M√©todo dos potenciais
  - An√°lise Financeira/M√©todo cont√°vel

---

- Vamos realziar $n$ opera√ß√µes de incremento.
- Calcular o n√∫mero de cada chamada √© complicado.
- Vamos pensar em quantas vezes cada bit √© trocado de 0 para 1 ou vice versa.
- Seja $F(i)$ o n√∫mero de vezes que o bit na posi√ß√£o $i$ √© flipado.

##### Contador Bin√°rio - $F(0)$

- $F(0) = n$
- ...

##### Contador Bin√°rio - $F(1)$

- $F(1) = \frac{n}{2}$
- ...

- $F(2) = \frac{n}{4}$
- $F(3) = \frac{n}{8}$
- ...
- $F(k) = \frac{n}{2^k}$
- ...
- $F(n - 1) = 2$ "Ele volta pra zero" [JV: N√£o entendi muito bem] "Esse $n$ √© relacionado ao tamanho do vetor", aqueles $n$'s acima s√£o da quantidade de opera√ß√µes.

$T(n) = \sum_{i=0}^{n-1} F(i) = \sum_{i=0}^{n-1} \frac{n}{2^i} = n \sum_{i=0}^{n-1} \frac{1}{2^i} \leq 3n$ "N√£o precisa responder isso exatamente"

$\frac{T(n)}{n} \leq \frac{3n}{n} \leq 3 = \Theta(1)$

[JV: Ele se embolou um pouco na nota√ß√£o dos √≠ndices e quantidades de opera√ß√µes]

Na teoria, deveria ser $\lim_{n \cond \inf} \frac{T(n)}{n}$, mas ele vai ignorar o limite.

##### M√©todo do Potencial

- VAmos atribuir uma energia potencial para a estrutura de dados.
- Temos uma fun√ß√£o que calcula esse potencial $\phi()$.
- Seja $D_0$ uma estrutura de dados, temos $\phi(D_0)$.
- Vamos realizar $n$ opera√ß√µes com essa estrutura.
- Seja $D_i$ √© a estrutura ap√≥s a i-√©sima opera√ß√£o.

---

... [N√£o anotei]

---

... [N√£o anotei]

Consideramos que a energia potencial √© a quantidade de bits que est√£o em 1.

---

- $b_0 = 0$ e $b_n = k$
- $b_i = b_{i-1} - t_i + 1$ isso significa que $b_i - b_{i-1} = -t_i + 1$
- $c^{\^}_i = c_i + \Phi(D_i) - \Phi(D_{i-1}) = t_i + 1 + (-t_i + 1) = 2$

D√∫vida Manu: de que forma √© definido o que √© uma opera√ß√£o?

##### M√©todo Cont√°vel

- Vamos analisar o nosso contador Bin√°rio.
- Vamos modificar os custos do nosso programa.
- Vamos assumir que flipar um bit para 1 custa 2.
- Por que?
- Porque vamos mudar umbit para 0 de gra√ßa!

A l√≥gica disso √© que em cada execu√ß√£o da fun√ß√£o, apenas h√° uma mudan√ßa de 0 para 1, ent√£o, cada uma das opera√ß√µes custa exatamente 2.

Esse √∫ltimo m√©todo nem sempre √© o mais direto, mas deve-ser ter cuidado na hora de se equilibrar as opera√ß√µes.

"Pode sempre ter lucro, o problema √© ficar no vermelho"

---

Fim da mat√©ria da P1

### Aula 6.1 (Extra) - 19/10/2024 - [09h03, 12h00]

#### Lista 1

##### Exerc√≠cio 15

- √â adequado recordar:
  - s√©rie de Taylor
    - $f(x) = \frac{1}{?} f(x_{1}^{i} + \frac{1}{?} G)$
  - Teorema de Stirling
    - $n^{\ln} = n^n$

##### **Exerc√≠cio 14.** Prove que $\sum^{n}_{i=1} i = \Theta (n^2)$, utilizando uma prova por indu√ß√£o

- $\sum^{n}_{i=1} i = O(n^2)$ || $\sum^{n}_{i=1} i = \Omega(n^2)$
  - $\sum^{n}_{i=1} i = O(n^2)$
    - [JV: Primeiro precisa escolher um $c$ e um $n_0$ que ser√£o usados por toda a prova indutiva.]
    - Ele escolheu $c = 3$ e $n_0 = 1$
    - Base: ($n_0 = 1$)
      - $1 \leq 3*(1)^2$ - OK
    - Hip√≥tese de indu√ß√£o: ($n = k$)
      - $\sum^{k}_{i=1} i \leq k^2$ - [No geral seria isso]
      - $\sum^{k}_{i=1} i \leq C*k^2$ - [Mas usamos a constante $C$ escolhida durante a prova por indu√ß√£o]
      - $\sum^{k}_{i=1} i \leq 3*k^2$ - [O $C$ escolhido foi 3]
    - Passo: ($n = k + 1$)
      - $\sum^{k+1}_{i=1} i = O(3*(k+1)^2)$
        - $\sum^{k+1}_{i=1} i = \sum^{k}_{i=1} i + (k+1) \leq 3*k^2 + (k+1) \leq 3*k^2 + 3(k+1) = 3(k^2 + k + 1) \leq 3(k+1)^2$
          - Primeiro ele removeu o √∫ltimo termo do somat√≥rio; depois ele fez a mesma soma de (k+1) no lado direito da hip√≥tese; E ent√£o foi trabalhando no lado direito da inequa√ß√£o at√© chegar no lado direito do passo inicial.
          - Obs.: $\sum^{k+1}_{i=1} i = \sum^{k}_{i=1} i + (k+1)$ [JV: Isso s√≥ √© verdade caso $N_0$ tenha pelo menos uma unidade, porque sen√£o o $k+1$ n√£o existiria]
  - $\sum^{n}_{i=1} i = \Omega(n^2)$
    - Ele n√£o vai fazer porque √© praticamente a mesma coisa que o anterior.
    - Para fazer o $\Omega$ eu poderia usar outros $C$ e $N_0$. Se eu fosse fazer a prova do Theta de uma vez s√≥, eu teria que escolher $C_1$, $C_2$ e $N_0$ para o $\Omega$ e para o $O$.

##### **Exerc√≠cio 11.** Dadas fun√ß√µes $f(n)$, $h(n)$ e $g(n)$ prove que

###### 11.1. Se $f(n) = O(g(n))$ e $g(n) = O(h(n))$ ent√£o $f(n) = O(h(n))$

- $\exists n_{0}^{f}, C^f$ tal que $f(n) \leq C^f * g(n)$ para todo $n \geq n_{0}^{f}$
- $\exists n_{0}^{g}, C^g$ tal que $g(n) \leq C^g * h(n)$ para todo $n \geq n_{0}^{g}$
  - $C^f * g(n) \leq C^f * C^g * h(n) | n \geq \max(n_{0}^{f}, n_{0}^{g})$ [Multiplicando g(n) por $C^f$ nos dois lados]
    - Obs.: N√£o precisa ser o m√°ximo, podemos somar ambos para simplificar.
  - $C^f * g(n) \leq C^f * C^g * h(n) | n \geq n_{0}^{f} + n_{0}^{g}$ ["Plugando" a inequa√ß√£o de f(n) em g(n)]
  - $f(n) \leq C^f * C^g * h(n) | n \geq n_{0}^{f} + n_{0}^{g}$ [Removendo o $g(n)$ da inequa√ß√£o]
  - Podemos agora dizer que $C = C^f * C^g$ e $n_0 = n_{0}^{f} + n_{0}^{g}$, com isso, podemos dizer que:
  - $f(n) \leq C * h(n) | n \geq n_0$

###### 11.2. $f (n) = O(f (n))$

- $N_0 = 1; C = 2$
  - $f(1) \leq 2*f(1)$
  - $f(n) \leq 2*f(n); n \leq n_0$

###### 11.3. Se $f (n) = \Omega(g(n))$ e $g(n) = \Omega(h(n))$ ent√£o $f (n) = \Omega(h(n))$

Acho que ele falou dessa, mas n√£o prestei aten√ß√£o ü´£.

###### 11.4. $f (n) = \Omega(f (n))$

Acho que ele falou dessa, mas n√£o prestei aten√ß√£o ü´£.

###### 11.5. $f (n) \neq o(f (n))$

Se n√£o me engano ele falou para tentar provar que isso √© correto e rapidamente voc√™ chega na conclus√£o de que isso √© absurdo.

##### **Exerc√≠cio 13.** Prove que $n \neq O(\log n)$

Prova por absurdo: negar a afirma√ß√£o e chegar numa contradi√ß√£o.

- $n = O(log n)$
  - $\exists n_0, C$ tal que $n \leq C * \log n | n \geq n_0$
  - $2^n \leq 2^{C * \log n} = 2^{log n^C} = n^C$
  - $2^n \leq n^C \implies \frac{2^n}{n^C} \leq 1$
    - "Isso daqui √© completamente absurdo, o que prova que $n = O(log n)$ √© falso, logo $n \neq O(log n)$" √© verdadeiro.

> Normalmente quando eu te mostrar uma desigualdade, geralmente a prova mais comum √© por absurdo.
>
> Provar ao escolher um $C$ e um $n_0$ que satisfa√ßam a equa√ß√£o n√£o √© a forma adequada de provar esse caso de desigualdade. Preciso provar que n√£o existe um $C$ e um $n_0$ que satisfa√ßam a equa√ß√£o.

---

> Se voc√™ for usar alguma propriedade bem espec√≠fica, te pe√ßo para que primeiro prove que essa sua propriedade seja verdadeira.

##### **Exerc√≠cio 12.** Prove que $n^3 \neq O(n^2)$

- Por absurdo:
  - $n^3 = O(n^2)$
    - $\exists n_0, C$ tal que $n^3 \leq C * n^2 | n \geq n_0$
    - $n \leq C | n \geq n_0$
      - Isso √© absurdo, logo $n^3 \neq O(n^2)$

##### **Exerc√≠cio 10.** Determine uma equival√™ncia assint√≥tica para as fun√ß√µes abaixo

- $\Omega(n^k) = n^k = O(n^k)$
- $\omega(n^{k-1}) = n^k = o(n^{k+1})$

###### 10.5. $4^n + 2^n + n$

- $4^n + 2^n + n = O(4^n)$ =? $O(2^n)$

- $4^n = O(2^n)$?
- $(2^2)^n = O(2^n)$?
- $2^{2n} = O(2^n)$?

- $2^{2n} = c*2^n | n \geq n_0$
- $\frac{2^{2n}}{2^n} = \frac{c*2^n}{2^n} = c$
- $2^n = c$?
  - N√£o.
  
- $a^n = O(b^n) se a \leq b$

"Pra pot√™ncia eu olho a base, pra polin√¥mio eu olho o expoente, pra logaritmo eu n√£o preciso"

- $\log_a(n) = \Theta(\log_2(n)); para a > 1$

Prova: mudan√ßa de base no log

- $\log_a(n) * \frac{1}{\log_a(2)} = \log_2(n)$

---

No geral ele deseja o mais apertado, mas √†s vezes resolver o especificamente mais apertado √© mais complicado.

Ent√£o por exemplo

Fibonacci: $Fib(n) = Fib(n-1) + Fib(n-2)$, qual √© o $O(Fib)$?

"√Ä rigor", o $O(Fib)$ √© $O((1+\phi)^n)$, mas ele aceitaria $O(2^n)$.

---

Quando ele s√≥ pede para dizer o limite de uma fun√ß√£o, ele n√£o espera que se prove. Ele apenas espera que se diga o limite.

##### Q2

Insertion
Melhor: O(n)
Pior caso: invertido

##### Q3

Bubble sort
Pior caso: invertido

---

Ele n√£o vai exigir a justificativa da complexidade, mas sugere que justifique. Porque com a justificativa ele pode dar alguma pontua√ß√£o pelo racioc√≠nio.

---

Quest√µes 1 e 2 s√≥ servem pra ilustrar o que √© essa c√°lculo de complexidade, mas esse somat√≥rio mi√∫do de opera√ß√µes n√£o √© a prioridade do momento. Ele n√£o vai pedir essa fun√ß√£o expl√≠cita do n√∫mero de passos espec√≠fico.

"Eu nunca vou te pedir o caso m√©dio... a n√£o ser que eu elabore a prova de uma forma muito esquisita"

---

#### Lista 2

Eu perguntei sobre a ideia de que $T(n) = 2T(n/2)$ serem duas chamadas com metade dos $n$ valores, ent√£o manteria sempre a mesma quantidade de valores sendo processados em todas as chamadas, e se com isso daria pra ter algum tipo de no√ß√£o intuitiva.

Ele comentou que toda vez que tu t√° multiplicando uma recorr√™ncia, j√° d√° pra entender que no final ter√° uma exponencial.

Entendo eu, ent√£o, que daria para ir analisando a recorr√™ncia em duas partes.

##### **Exerc√≠cio 1.** Determine e prove uma equival√™ncia assint√≥tica para todas as recorr√™ncias abaixo

###### 2. $T (n) = 2T (n - 2) + \log n$

- $T (n  ) = 2T (n - 2) + \log n$
- $T (n  ) = 2(2T ((n-2) - 2) + \log (n-2)) + \log n$
  - $T (n  ) = 2^2T (n - 4) + 2\log (n-2) + \log n$
- $T (n  ) = 2^2(2T ((n-4) - 2) + \log (n-4)) + 2\log (n-2) + \log n$
  - $T (n  ) = 2^3T (n - 6) + 2^2\log (n-4) + 2\log (n-2) + \log n$
- $\vdots$
- $T (n  ) = 2^kT (n - 2k) + \sum_{i=0}^{k-1} 2^i\log (n-2i)$
- $\vdots$ Eq 1.2:
- $T(n) = 2^{n/2}*\Theta(1) + \sum_{i=0}^{n/2 - 1} 2^i\log (n-2i)$
- $T(n) = \Theta(2^{n/2}) + \dots$
  - $\dots = \sum_{i=0}^{n/2 - 1} 2^i\log (n-2i) \leq \sum_{i=0}^{n/2 - 1} 2^i\log n$
  - $\dots = \log n \sum_{i=0}^{n/2 - 1} 2^i = (2^{n/2} - 1) \log n = O(2^{n/2} * \log n)$

---

$$
\text{Eq 1.2:}\\
T(n - 2k) = T(0)\\
n-2k = 0\\
n = 2k\\
k = \frac{n}{2}\\
$$

---

Sugest√£o: tentar fazer essa indu√ß√£o pro caso base do teorema mestre $T(n) = aT(\frac{n}{b}) + n^k$

#### **Exerc√≠cio 3.** Usando o teorema mestre determine uma equival√™ncia assint√≥tica para

##### 1. $T (n) = 2T ( \frac{n}{4} ) + 1$

- $a = 2; b = 4; \log_{b}(a) = \log_{4}(2) = \frac{1}{2}$
- $n^{\log_{b}(a)} = n^{\frac{1}{2}} = \sqrt{n}$
- $f(n) = 1
- 1¬∫ caso: $\Theta(\sqrt{n})$

Se $T(n) = 2T(\frac{n}{4}) + \sqrt{n}$, ent√£o: 2¬∫ caso: $\Theta(\sqrt{n}*\log n)$
Se $T(n) = 2T(\frac{n}{4}) + n$, ent√£o: 3¬∫ caso: $\Theta(n)$
Se $T(n) = 2T(\frac{n}{4}) + \sqrt{n}*\log n$, ent√£o: 4¬∫ caso: n√£o d√°!

---

##### **Exerc√≠cio 2.** Determine e prove uma equival√™ncia assint√≥tica para todas as recorr√™ncias abaixo. **N√£o use o teorema mestre**

###### 2. $T (n) = 4T ( \frac{n}{2} ) + \log n$

- $T (n) = 4T ( \frac{n}{2} ) + \log n$
- $T (n) = 4(4T ( \frac{n}{4} ) + \log \frac{n}{2}) + \log n$
  - $T (n) = 4^2T ( \frac{n}{8} ) + 4\log \frac{n}{2} + \log n$
- $T (n) = 4^2(4T ( \frac{n}{8} ) + \log \frac{n}{4}) + 4\log \frac{n}{2} + \log n$
  - $T (n) = 4^3T ( \frac{n}{16} ) + 16\log \frac{n}{4} + 4\log \frac{n}{2} + \log n$
- $\vdots$
- $T (n) = 4^kT ( \frac{n}{2^k} ) + \sum_{i=0}^{k-1} \log \frac{n}{2^i} * 4^i$
- $\vdots$ Eq 2.1:
- $T(n) = [4^{\log_2(n)}*\Theta(1)] + [\sum_{i=0}^{\log_2(n) - 1} 4^i * \log \frac{n}{2^i}]$
  - $[4^{\log_2(n)}*\Theta(1)] \dots$
    - $[4^{\log_2(n)}] =$
    - $(2*2)^{\log_2(n)} =$
    - $2^{\log_2 n} * 2^{\log_2 n} =$
    - $n^2$
    - $O(n^2)$
  - $\dots + [\sum_{i=0}^{\log_2(n) - 1} 4^i * \log \frac{n}{2^i}]$
    - $\sum_{i=0}^{\log_2(n) - 1} 4^i * \log \frac{n}{2^i} =$
    - $\log n \sum_{i=0}^{\log_2(n) - 1} 4^i =$
    - $\log n * (\frac{4^{\log_2(n)} - 1}{3}) =$
    - $\frac{\log n * n^2}{3} =$
    - $O(n^2 * \log n)$

---

$$
\text{Eq 2.1:}\\
T(\frac{n}{2^k}) = T(1)\\
\frac{n}{2^k} = 1\\
n = 2^k\\
k = \log_2(n)\\
$$

---

##### Complexidade Amortizada

Para as quest√µes a seguir considere uma pilha S que possui duas opera√ß√µes

- **pop(S)**: remove (desempilha) o topo da pilha S.
- **push(S,x)**: empilha o elemento x na pilha S.

Cada uma dessas opera√ß√µes possui custo O(1). Vamos definir uma nova opera√ß√£o para esta estrutura, a opera√ß√£o **multi-pop(S,k)** que remove os k √∫ltimos elementos empilhados.

---

###### **Exerc√≠cio 8.** Qual a complexidade amortizada da opera√ß√£o de **multi-pop** dada uma sequ√™ncia de opera√ß√µes de push, pop e **multi-pop** em uma pilha originalmente vazia?

- *multi-pop(S, k)*:
  - p = k;
  - **Enquanto** !*vazio*(S) **e** p > 0:
    - *pop*(s)

- Pior caso: $O(k)$
- Melhor Caso: $\Theta(1)$
- Complexidade amortizada: $\Theta(1)$
  - Mudam-se os valores: *push()* passa a ter custo 2 e *pop()* passa a ter custo 0.
  - Dessa forma, para os n passos de *push()* e *pop()* temos um custo total de 2n.
  - Fazendo a divis√£o do custo pela quantidade de passos, temos que $\frac{2n}{n} = 2 = \Theta(1)$

Geralmente essa din√¢mica de trocar a quantidade de opera√ß√µes j√° √© suficiente como prova

##### **Exerc√≠cio 9.** Qual o custo computacional de sequ√™ncia de n opera√ß√µes de **push**, **pop** e **multi-pop** em uma pilha com inicialmente $s_O$ elementos e que termina com $s_n$ elementos?

Mesmo argumento que o anterior, mas com an√°lise de potencial.

Ele recomenda usar a fun√ß√£o de potencial como sendo a quantidade de elementos

### D√∫vidas M√≥dulo 1

$O(n!)$ isn't equivalent to $O(n^n)$. It is asymptotically less than $O(n^n)$.

$O(\log(n!))$ is equal to $O(n \log(n))$. Here is one way to prove that:

Note that by using the log rule $\log(mn) = \log(m) + \log(n)$ we can see that:

$\log(n!) = \log(n*(n-1)*...2*1) = \log(n) + \log(n-1) + ... \log(2) + \log(1)$

Proof that $O(\log(n!)) \subseteq O(n \log(n))$:

$\log(n!) = \log(n) + \log(n-1) + ... \log(2) + \log(1)$

Which is less than:

$\log(n) + \log(n) + \log(n) + \log(n) + ... + \log(n) = n*\log(n)$

So $O(\log(n!))$ is a subset of $O(n \log(n))$

Proof that $O(n \log(n)) \subseteq O(\log(n!))$:

$\log(n!) = \log(n) + \log(n-1) + \dots \log(2) + \log(1)$

Which is greater than the left half of that expression with all $(n-x)$ replaced by $n/2$:

$\log(n/2) + \log(n/2) + ... + \log(n/2) = floor(n/2)*\log(floor(n/2)) \in O(n \log(n))$

So $O(n \log(n))$ is a subset of O(\log(n!))$.

Since $O(n \log(n)) \subseteq O(\log(n!)) \subseteq O(n \log(n))$, they are equivalent big-Oh classes.

## M√≥dulo 2

### Aula 5 - 14/10/2024 - [13h10, 14h40]

#### Slide - Grafos e Representa√ß√£o

- **Grafo**: $G = (V, E)$
  - $V$: conjunto de v√©rtices
    - $|V(G)| = n$
  - $E$: conjunto de arestas
    - $|E(G)| = m$
- **Vizinhan√ßa**: $N(v) = \{u | (u, v) \in E\}$ [JV: Todos os v√©rtices conectados ao v√©rtice $v$]
- **Grau**: $d(v) = |N(v)|$ [JV: Quantos v√©rtices est√£o conectados ao v√©rtice $v$]
- **Caminho**: conjunto de arestas $(v_1, v_2), \dots, (v_{k-1}, v_k)$ que n√£o passa duas vezes pelo mesmo v√©rtice.
- **Ciclo**: conjunto de arestas $(v_1, v_2), \dots, (v_{k-1}, v_k)$ que n√£o passa duas vezes pelo mesmo v√©rtice, exceto o primeiro e o √∫ltimo.
  - Ele considerar√° como ciclo o conjunto dos v√©rtices, n√£o se importando ent√£o com a ordem das arestas.

---

<!-- - Subgrafo: $G' = (V', E')$ tal que $V' \subseteq V$ e $E' \subseteq E$ -->

---

Representa√ß√£o

```mermaid
graph TD;
    1---2;
    1---3;
    1---4;
    2---4;
    3---4;
    5---4;
    5---2;
    4---6;
```

- $V = \{1, 2, 3, 4, 5, 6\}$
- $E = \{(1, 2), (1, 3), (1, 4), (2, 4), (3, 4), (5, 4), (5, 2), (4, 6)\}$

---

- **La√ßo**: quando uma aresta vai de um v√©rtice para ele mesmo.
- **Grafo simples**: n√£o tem la√ßos nem arestas m√∫ltiplas. [JV: ele nem considera que h√° duas arestas entre dois mesmos v√©rtices. Nesse caso passa a ser um "multigrafo"] [JV: o la√ßo √© uma aresta que vai do v√©rtice a ele mesmo]
  - Nesses casos um v√©rtice n√£o faz parte de sua pr√≥pria vizinhan√ßa.

$$
\sum_{v \in V(G)} d(v) = 2|E(G)| = 2m = \Theta(m)
$$

- $\forall v \in V(G): \delta(G) \leq d(v) \leq \Delta(G)$
  - O maior dos graus do grafo: $\Delta(G) = \max_{v \in V(G)} d(v)$
  - O menor dos graus do grafo: $\delta(G) = \min_{v \in V(G)} d(v)$

- Grafo Conexo: eu consigo chegar em qualquer lugar, partindo de qualquer lugar.
  - D√∫vida: um grafo de um v√©rtice √© conexo.
- Grafo Desconexo: existem v√©rtices que n√£o conseguem se conectar a outros v√©rtices.
  - Componentes conexas: subgrafos conexos m√°ximos. [JV: o m√°ximo de v√©rtices que conseguem se conectar entre si]
  - Um grafo com v√©rtices e sem arestas tamb√©m seria um grafo desconexo.

- **Subgrafo**: qualquer peda√ßo de um grafo.
  - Para $G = (V, E)$ e $H = (V', E')$, $H$ √© subgrafo de $G$ se $V' \subseteq V$ e $E' \subseteq E$.

Curiosidade: podemos usar a nota√ß√£o de subconjunto para representar subgrafos. Ex.: $H \subseteq G \equiv \text{H √© subgrafo de G}$

- **Subgrafo Induzido**: subgrafo que mant√©m as arestas entre os v√©rtices do subconjunto. [JV: todas as arestas de um est√£o no outro, e n√£o existem arestas no induzido que n√£o estejam no original]
  - [Copilot: $H$ √© subgrafo induzido de $G$ se $V' \subseteq V$ e $E' = \{(u, v) \in E | u, v \in V'\}$]
  - H √© subgrafo induzido de G
    - $V(H) \subseteq V(G)$
    - $E(H) \subseteq E(G); vu \in E(H) \equiv vu \in E(G)$

Grafo ac√≠clico: n√£o tem ciclos como subgrafo.

Se o grafo for ac√≠clico e conexo, ele √© uma √°rvore.

Se $G = (V, E)$ √© uma √°rvore, $m = \Theta(n) = n - 1$.

**Floreta**: grafo desconexo em que cada componente conexo √© uma √°rvore.

**Complemento**: √© o grafo composto pelos v√©rtices que n√£o est√£o conectados no grafo original, ou seja, "o que est√° faltando de um grafo para ser completo".

- Complemento de $G = (V, E)$ => $ \overline{G} = (V, \overline{E})$ onde:
  - $V = V(\overline{G})$
  - $E(\overline{G}) = \overline{E(G)} - \{uu | u \in E(G)\}$

##### Grafos - Representa√ß√£o

1. ...
2. ...

##### Representa√ß√£o - Matriz de Adjac√™ncia

- Uma matriz onde a primeira linha e a primeira coluna possuem os nomes dos v√©rtices.
- O valor $a_{ij}$ √© 1 se $v_i$ e $v_j$ s√£o adjacentes e 0 caso contr√°rio.

$$
\begin{bmatrix}
  X & 1 & 2 & 3 & 4 & 5 & 6\\
  1 & 0 & 1 & 1 & 1 & 0 & 0\\
  2 & 1 & 0 & 0 & 1 & 0 & 1\\
  3 & 1 & 0 & 0 & 1 & 0 & 0\\
  4 & 1 & 1 & 1 & 0 & 1 & 0\\
  5 & 0 & 0 & 0 & 1 & 0 & 0\\
  6 & 0 & 1 & 0 & 0 & 0 & 0\\
\end{bmatrix}
$$

Se a diagonal principal √© 0, significa que n√£o tem la√ßo.
Se ele n√£o √© direcionado, significa que ele √© sim√©trico, ou seja, sua transposta √© identica.

##### Representa√ß√£o - Lista de Adjac√™ncia (Lista encadeada)

- adj[1] = [2, 3, 4]
- adj[2] = [1, 4, 6]
- adj[3] = [1, 4]
- adj[4] = [1, 2, 3, 5]
- adj[5] = [4]
- adj[6] = [2]

- "E qual √© a complexidade para achar se a aresta $(4, 5)$ existe?" $(4, 5) \in G = (V, E)?$
  - Lista de adjac√™ncia: $ O(\Delta(G)) $
  - Matriz: $ O(1) $
- "E qual √© a complexidade para achar os vizinhos de G?" $N(v) = \{u \in V(G) | uv \in E(G)\}$
  - Lista de adjac√™ncia: $ O(1) $
  - Matriz: $ O(n) $
- Grau: $d(v) = | N(v)$
  - Lista de adjac√™ncia: $ O(v)) $
  - Matriz: $ O(n) $
- Orrdene $V(g)$ por grau;
  - $O(n \log n)$
  - $\sum_{v \in V(G)} O(d(v)) = \Theta(m * n \log n)$

---

#### Respresenta√ß√£o 2

- Grafo direcionado:

```mermaid
graph TD;
    1-->2;
    1-->3;
    1-->4;
    2-->4;
    2-->5;
    4-->5;
    3-->4;
    5-->4;
    5-->2;
```

- Vizinhan√ßa de Entrada: $N^-(v) = \{u | (u, v) \in E\}$
- Vizinhan√ßa de Sa√≠da: $N^+(v) = \{u | (v, u) \in E\}$
- Grau de Entrada: $d^-(v) = |N^-(v)|$
- Grau de Sa√≠da: $d^+(v) = |N^+(v)|$

- Se de toda aresta der para chegar em qualquer uma outra, diz-se que o grafo √© fortemente conexo.
- Todo v√©rtice √© subconexo consigo mesmo.
- Em grafos orientados, grafo ac√≠clico √© quando n√£o tem ciclo orientado. E pode ser chamado de DAG (Directed Acyclic Graph).

Poderia-se guardar apenas as arestas de sa√≠da, afinal o de entrada seria praticamente a duplicada de dire√ß√£o oposta do grafo origiinla.

$G = (V, A)$, A = arcos [JV: n√£o entendi qual a diferen√ßa das arestas// o Arco √© a aresta direcionada]

De que forma colocar o peso das arestas nas representa√ß√µes em matriz e lista sem alterar as complexidades?

### Aula 6 - 16/10/2024 - [13h11, 14h40]

- A aula de exerc√≠cios sobre as duas primeiras listas ser√° no s√°bado 2015 ICEx de 9h √†s 12h.
- Ele vai passar um c√≥digo no Moodle para entrar na disciplina do BeeCrowd para poder fazer os trabalhos.
- "Juizes online" (?)
- Trabalho pr√°tico limitado a C, C++, Python e Java.
- Eles v√£o penalizar por uso do GPT e por pl√°gio do colega.
- Se atentar √† formata√ß√£o esperada pelo sistema.
- At√© o final do dia da segunda prova a gente j√° tem que ter terminado todos os exerc√≠cios.
- Cada exerc√≠cio vai valer 0,5
- Muitas vezes as quest√µes precisariam de alguma coisa muito espec√≠fica para serem resolvidas, e por isso ser√£o ignoradas.
- Cuidado com dicion√°rios em Python.
- Pode usar biblioteca? No geral a gente n√£o vai precisar usar.
- "O objetivo √© ordenar um vetor, voc√™ vai l√° e usar o sort do Python. N√£o √© o objetivo."
- Se a fun√ß√£o auxiliar n√£o for o principal do algoritmo, n√£o tem problema usar.
- De modo geral, √© esperado que o c√≥digo tenha mais do que duas linhas.
- Aula monitora: quinta feira de 18h30 √†s 20h30.

#### Slide - Corretude de Algoritmos

##### Corretude de Algoritmos

- √â necess√°rio mostrar que nossos algoritmos s√£o corretos.

##### Soma de elementos num vetor

- "Deveria a rigor mostrar matematicamente que esse algoritmo faz o que diz"

Um dos m√©todos se chama "invariante de la√ßo" para algoritmos que n√£o s√£o recursivos.

- **Invariante de La√ßo**: √© uma propriedade que √© verdadeira antes e depois de cada itera√ß√£o do la√ßo.

No caso desse algoritmo, a invariante √© que a soma de todos os elementos do vetor, at√© aquela itera√ß√£o: $Soma = \sum_{i=1}^{n} x[i]$

Geralmente esse tipo de ideia t√° acompanhada de uma prova por indu√ß√£o. E isso √© bem mais comum em algoritmos recursivos.

##### Fatorial

- **Base:** $k = 1$
  - fat(1) = 1 = 1! OK
- **Hip√≥tese:** $fat(k) = k!$
- **Passo:** $fat(k+1) = (k+1)!$
  - $fat(k+1) = (k+1) * fat(k) = (k+1) * k! = (k+1)!$

A rigor ele deveria mostrar que o algoritmo faz o que diz. Ele vai fazer isso? N√£o.

"T√° bem na cara que o fatorial √© um algoritmo recursivo que faz o que faz"

Quando o algoritmo for absurdamente dif√≠cil, ele n√£o vai provar que ele t√° correto justamente por precisar de muitos passos pra mostrar isso.

"Se for muito dif√≠cil vou simplesmente pedir para que tenham f√©."

#### Slide - DFS e Aplica√ß√µes

Busca em profundidade

##### Busca em Grafos

- Visitar os v√©rtices do grafo em ordem.
- Necessidade em muitos algoritmos.
- Principal opera√ß√£o de uma estrutura de dados.
- Propriedades distintas.
- Aplic√°vel a grafos e d√≠grafos. [JV: d√≠grafos s√£o grafos direcionados]

A ideia √© ir o mais distante poss√≠vel e visitar primeiro os filhos e por √∫ltimo os pais.

##### Busca em Profundidade

- Vamos visitar os v√©rtices filhos de um n√≥ antes dele.
- Vamos manter algumas vari√°veis:
  - $\pi$: pai de cada v√©rtice $v$.
    - Como cada um s√≥ tem um pai, os grafos n√£o ser√£o c√≠clicos.
  - $i[v]$: tempo que encontramos o v√©rtice $v$. [Quando eu chego]
  - $f[v]$: tempo que visitamos o v√©rtice $v$. [Quando eu saio]
- V√©rtices brancos, cinzas e pretos.
  - Brancos: n√£o encontrados.
  - Cinzas: encontrados, mas ainda h√° algo faltante.
  - Pretos: j√° terminou de caminhar com a busca.

##### Busca em Profundidade - Algoritmo

Corre√ß√µes:

- $\pi[u] = \lambda$ -> $\pi[v] = v$
- Um def de alguma coisa u, deveria ser V

---

Se for um grafo direcionado, precisa indicar que vai percorrer pela vizinhan√ßa de sa√≠da.

##### Busca em Profundidade - Exemplo

[JV: os labels de cada v√©rtice indicam o tempo de entrada e sa√≠da, ou seja: $i[v]$ e $f[v]$]

```mermaid
graph TD
    1-->2
    1-->3
    1-->4
    2-->1
    2-->4
    3-->1
    3-->4
    3-->6
    4-->5
    4-->6
    5-->4
```

Matriz de adjac√™ncia:

$$
\begin{bmatrix}
  X & 1 & 2 & 3 & 4 & 5 & 6\\
  1 & 0 & 1 & 1 & 1 & 0 & 0\\
  2 & 1 & 0 & 0 & 1 & 0 & 0\\
  3 & 1 & 0 & 0 & 1 & 0 & 1\\
  4 & 1 & 1 & 1 & 0 & 1 & 1\\
  5 & 0 & 0 & 0 & 1 & 0 & 0\\
  6 & 0 & 0 & 1 & 1 & 0 & 0\\
\end{bmatrix}
$$

Outra ordena√ß√£o da Matriz de Adjac√™ncia:

$$
\begin{bmatrix}
  X & 6 & 5 & 4 & 3 & 2 & 1\\
  1 & 0 & 0 & 1 & 1 & 1 & 0\\
  2 & 0 & 0 & 1 & 0 & 1 & 1\\
  3 & 1 & 0 & 1 & 0 & 1 & 1\\
  4 & 1 & 1 & 0 & 1 & 1 & 1\\
  5 & 0 & 0 & 1 & 0 & 0 & 0\\
  6 & 0 & 0 & 1 & 1 & 0 & 0\\
\end{bmatrix}
$$

- Intervalos:
  - 1: $[1, 12]$
  - 2: $[2, 9]$
  - 3: $[10, 11]$
  - 4: $[3, 8]$
  - 5: $[4, 5]$
  - 6: $[6, 7]$
- Outra representa√ß√£o:
  - (1(2(4(5)(6))(3))

- Curiosidades:
  - Os intervalos dos filhos est√£o dentro dos intervalos dos pais.
  - Os que n√£o t√™m intervalos sobrepostos, s√£o irm√£os (?) [JV: √© mais ou menos isso]
  - Isso √© o teorema dos par√™nteses. "Criando parentesco entre os v√©rtices"

N√£o faz sentido haver intervalos tipo: [1, 3] e [2, 4] porque isso significaria que o filho terminou de ser processado depois do pai.

---

Come√ßando por v√©rtices diferentes, geram-se diferentes √°rvores.

[JV: √°rvore de predecessores] O que √©? Copilot: √© uma √°rvore que indica quem √© o pai de cada v√©rtice.

---

Qual seria a complexidade?

Se for matriz: $|V| + |V| * O (|V|) = |V| + O(|V|^2) = O(|V|^2)$; Considerando que $|V| = n: O(n^2)$

Se for lista: $|V| + \sum_{v \in V} O(d(v))$; $|E| = m$; $O(n) + \Theta(m) = O(n + m)$
Se o grafo for uma √°rvore: $O(n + n) = O(n)$

Matematicamente seria:

- Lista: $F(|V|) = F(|V| - 1) + 1 + \Theta(d(v))$
- Matriz: $F(|V|) = F(|V| - 1) + 1 + \Theta(n)$

- Lista: $O(n) + O(m)$
- Matriz: $O(n) + O(n^2)$

---

$m \leq n^2$

Relembrando: $m = |E|$; $n = |V|$

Geralmente ele vai querer as f√≥rmulas tanto da lista quanto da matriz.

## Estudar

- Material de Pr√©-PAA
- Heap
- Transforma√ß√£o de logaritmo
- Propriedades de logaritmo
- Soma de PA e PG

## D√∫vidas

$O(n!)$ isn't equivalent to $O(n^n)$. It is asymptotically less than $O(n^n)$.

$O(\log(n!))$ is equal to $O(n \log(n))$. Here is one way to prove that:

Note that by using the log rule $\log(mn) = \log(m) + \log(n)$ we can see that:

$\log(n!) = \log(n*(n-1)*...2*1) = \log(n) + \log(n-1) + ... \log(2) + \log(1)$

Proof that $O(\log(n!)) \subseteq O(n \log(n))$:

$\log(n!) = \log(n) + \log(n-1) + ... \log(2) + \log(1)$

Which is less than:

$\log(n) + \log(n) + \log(n) + \log(n) + ... + \log(n) = n*\log(n)$

So $O(\log(n!))$ is a subset of $O(n \log(n))$

Proof that $O(n \log(n)) \subseteq O(\log(n!))$:

$\log(n!) = \log(n) + \log(n-1) + \dots \log(2) + \log(1)$

Which is greater than the left half of that expression with all $(n-x)$ replaced by $n/2$:

$\log(n/2) + \log(n/2) + ... + \log(n/2) = floor(n/2)*\log(floor(n/2)) \in O(n \log(n))$

So $O(n \log(n))$ is a subset of O(\log(n!))$.

Since $O(n \log(n)) \subseteq O(\log(n!)) \subseteq O(n \log(n))$, they are equivalent big-Oh classes.
## Estudar

- Material de Pr√©-PAA
- Heap
- Transforma√ß√£o de logaritmo
- Propriedades de logaritmo
- Soma de PA e PG
